{:splitter
 {:type :recursive
  :params {:size 1000
           :overlap 20}}

 :embedder
 {:type :all-mini-lm-l6-v2
  :params {}}

 :vector-store
 {:type :in-memory
  :params {}}

 :memory
 {:type :window
  :params {:size 10}}

 :prompt-template
 {:type :default
  :params {}}

 :llm
 {:type :llama-cpp
  :params {:model {:type :hugging-face
                   :name "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
                   :file "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
                   :chat-template "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
                   :bos-token "<|begin_of_text|>"
                   :eos-token "<|end_of_text|>"}
           :n-ctx 8192}}}